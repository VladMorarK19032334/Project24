{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTR_Seg&KeypointLocal_ExperimentalEnvi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNBjmI3Fpxm9hftyUHTpHbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladMorarK19032334/Project24/blob/main/CTR_Seg%26KeypointLocal_ExperimentalEnvi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dBksUtWpNLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "LM9XKwqmdkV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_AUGMENTATION == 'complex': # due to piecewise affine => different import is required\n",
        "  !python -m pip install --upgrade opencv-contrib-python\n",
        "  !pip uninstall opencv-python\n",
        "  !pip install git+https://github.com/albumentations-team/albumentations\n",
        "  !pip install opencv-python\n",
        "else:\n",
        "  !pip install albumentations==0.4.6\n",
        "\n",
        "  import albumentations as A\n",
        "  from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "f2RF9BSSdwot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import cv2\n",
        "\n",
        "import statistics as stat\n",
        "import math\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "id": "PZau9FvLdmoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimental Parameters - to decide what experiment to run for"
      ],
      "metadata": {
        "id": "lN4zncVYcs8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3gkWNGMcAiT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# True - the loss has weights on classes, False - runs the segmentation for UNet with no weights\n",
        "# available for multi-learning as well\n",
        "WEIGHTED_UNET = True \n",
        "\n",
        "'''\n",
        "Multi-learning Network with Segmentation Feedback propagation\t\t\tMultiSFPNet\n",
        "Double Headed UNet\t\t\t                                          DH-Unet\n",
        "Multi-Learning Network Non-Connected Output\t\t  \t                MultiNCONet\n",
        "'''\n",
        "EXPERIMENTAL_MODEL = 'MultiSFPNet' # select the model to run\n",
        "\n",
        "'''\n",
        "none\t\t\tNo augmentation\n",
        "full\t\t\tThe full augmentation used in the latest models (more complex with no piecewise)\n",
        "complex\t\tFull augmentation with Piecewise Affine transformation\n",
        "'''\n",
        "MODEL_AUGMENTATION = 'none'\n",
        "\n",
        "\n",
        "# select training dataset\n",
        "LARGE_DATASET = False\n",
        "LARGE_IMAGESIZE = True\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 5\n",
        "NUM_EPOCHS = 100\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 480  # original size: 480px\n",
        "IMAGE_WIDTH = 640  # original size: 640px\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# segmentation weight loss\n",
        "SEG_LOSS_WEIGHT = 1e+4 # previously was 1e+4\n",
        "SEG_LOSS_WEIGHT_MIN = 1 # minimum value of loss\n",
        "SEG_LOSS_WEIGHT_FUNCTION = 'step' # step, linear, exp\n",
        "\n",
        "# weight for class segmentation in cross entropy loss\n",
        "CLASS_WEIGHTS = torch.tensor([0.5, 2.0, 1.0, 1.0]) # [bg, tip, middle, base]\n",
        "\n",
        "\n",
        "# form keypoint normalization\n",
        "KEYPOINT_NORM = False \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if LARGE_DATASET == True: \n",
        "  TRAIN_IMG_DIR = \"\"\n",
        "  TRAIN_MASK_DIR = \"\"\n",
        "  TRAIN_LOCAL_DIR = \"\"\n",
        "else:\n",
        "  TRAIN_IMG_DIR = \"\"\n",
        "  TRAIN_MASK_DIR = \"\"\n",
        "  TRAIN_LOCAL_DIR = \"\"\n",
        "\n",
        "\n",
        "VAL_IMAGE_DIR = \"\"\n",
        "VAL_MASK_DIR = \"\"\n",
        "VAL_LOCAL_DIR = \"\"\n",
        "\n",
        "# datasets for saving all necessary visual results\n",
        "GLOBAL_FOLDER = \"\" # location of envi savings\n",
        "EXPERIMENT_NAME = \"\"\n",
        "SAVE_FOLDER = f'{GLOBAL_FOLDER}/{EXPERIMENT_NAME}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Runner"
      ],
      "metadata": {
        "id": "Gu5XRHBwhhsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0E4UoNU67Vod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main algorithm run function\n",
        "\n",
        "def main():\n",
        "  _saved_image_index = 0\n",
        "  # select segmentation properties\n",
        "  if MODEL_AUGMENTATION == 'full': # full augmentation\n",
        "      train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Rotate(limit=90, p=1.0),\n",
        "              A.HorizontalFlip(p=0.5),\n",
        "              A.VerticalFlip(p=0.25),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "\n",
        "      val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "  elif MODEL_AUGMENTATION == 'complex': # complex augmentation\n",
        "    train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Rotate(limit=90, p=1.0),\n",
        "              A.HorizontalFlip(p=0.5),\n",
        "              A.VerticalFlip(p=0.25),\n",
        "              A.augmentations.geometric.transforms.PiecewiseAffine(scale=(0.03, 0.05), nb_rows=8, nb_cols=8, \n",
        "                                interpolation=1, mask_interpolation=0, cval=0, \n",
        "                                cval_mask=0, mode='constant', absolute_scale=False, \n",
        "                                always_apply=False, keypoints_threshold=0.01, p=0.2), # bilinear interpolation image, nearest neighbour for mask\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "  else: # none augmentation\n",
        "    train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ],\n",
        "          keypoint_params=A.KeypointParams(format='xy',remove_invisible=False)\n",
        "      )\n",
        "    \n",
        "\n",
        "  model = UNET(in_channels=3, out_channels=4, out_channels_local=8).to(DEVICE)\n",
        "  \n",
        "  # select segmentation properties\n",
        "  if WEIGHTED_UNET: # weighted classes\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(DEVICE))\n",
        "  else:\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_local = nn.MSELoss() # for keypoints\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_MASK_DIR,\n",
        "      TRAIN_LOCAL_DIR,\n",
        "      VAL_IMAGE_DIR,\n",
        "      VAL_MASK_DIR,\n",
        "      VAL_LOCAL_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transforms,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY,\n",
        "  )\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "      load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
        "  \n",
        "  accuracy(val_loader, model, device=DEVICE)\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "      seglossfactor(SEG_LOSS_WEIGHT_FUNCTION, SEG_LOSS_WEIGHT_MIN, epoch)\n",
        "\n",
        "\n",
        "      print(f'EPOCH: {epoch+0}')\n",
        "      torch.cuda.empty_cache() # empty cuda cache\n",
        "      train_fn(train_loader, model, optimizer, loss_fn, loss_local, scaler)\n",
        "\n",
        "      #save_loss_propagation(loss_arr, epochs=1)\n",
        "\n",
        "      # save model\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "      }\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "      # check accuracy\n",
        "      accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "      # print examples every 5 epochs\n",
        "      if _saved_image_index%10 == 0 or _saved_image_index == 0 or _saved_image_index == 99:\n",
        "        # print some examples to a folder\n",
        "        save_predictions_as_imgs(\n",
        "            val_loader, model, _saved_image_index, SAVE_FOLDER, device=DEVICE\n",
        "        )\n",
        "\n",
        "      _saved_image_index += 1\n",
        "        \n",
        "\n",
        "#torch.backends.cudnn.enabled = False\n",
        "# train model\n",
        "main()\n",
        "save_loss_propagation(loss_arr)\n"
      ],
      "metadata": {
        "id": "fyI7qjiMhqNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "W9frT7zgAoX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_arr = []\n",
        "\n",
        "loss_arr_epoch = []\n",
        "\n",
        "# does 1 Epoch of training\n",
        "def train_fn(loader, model, optimizer, loss_fn, loss_local, scaler):\n",
        "    loop = tqdm(loader) # progress bar\n",
        "\n",
        "    seg_loss_arr = []\n",
        "    local_loss_arr = []\n",
        "\n",
        "    loop_idx = 0 # how many time you went through data (for duplicated datasets)\n",
        "\n",
        "\n",
        "    for batch_idx, (data, mask_target, keypoints_target) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        mask_target = mask_target.long().to(device=DEVICE)\n",
        "        keypoints_target = keypoints_target.float().to(device=DEVICE)\n",
        "\n",
        "        if batch_idx%32 == 0 and batch_idx!=0: # check if run through all unique data\n",
        "          loop_idx += 1\n",
        "\n",
        "        # forward\n",
        "        losses = []\n",
        "        with torch.cuda.amp.autocast():\n",
        "            [output_mask, output_keypoints] = model(data)\n",
        "            #targets = targets.long().to(device=DEVICE)\n",
        "            loss1 = loss_fn(output_mask, mask_target)\n",
        "            loss2 = loss_local(output_keypoints.float(), keypoints_target)\n",
        "\n",
        "            if batch_idx > 14 + loop_idx*32: # if extra set available => cancel effects of segmentation loss as no mask is available (This way works only for the non-duplicated dataset)\n",
        "              losses.append(0*loss1)\n",
        "            else:\n",
        "              losses.append(SEG_LOSS_WEIGHT*loss1)\n",
        "\n",
        "            losses.append(loss2)\n",
        "            loss = sum(losses)\n",
        "\n",
        "            loss_arr_epoch.append(loss.detach().cpu())\n",
        "\n",
        "            # append to separate arrays\n",
        "            seg_loss_arr.append(float(SEG_LOSS_WEIGHT*loss1))\n",
        "            local_loss_arr.append(float(loss2))\n",
        "            \n",
        "\n",
        "        # backward'\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "    loss_arr.append(float(sum(loss_arr_epoch))/len(loss_arr_epoch))\n",
        "    # print losses\n",
        "    print(f\"\\n The segmentation loss is: {stat.mean(seg_loss_arr)}\")\n",
        "    print(f\"The localization loss is: {stat.mean(local_loss_arr)}\\n\")"
      ],
      "metadata": {
        "id": "FHzhldlQApWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation Loss Factor"
      ],
      "metadata": {
        "id": "_IeDbwEQbAWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "format - 'step', 'linear', 'exp'\n",
        "       - decides on seg loss decay factor\n",
        "'''\n",
        "def seglossfactor(format, min, epoch, seg=SEG_LOSS_WEIGHT):\n",
        "  \n",
        "  # step function\n",
        "  if format == 'step':\n",
        "      # set weight in terms of epoch from focus on seg to focus on local\n",
        "      if epoch == 10:\n",
        "        #SEG_LOSS_WEIGHT = 1e+4\n",
        "        pass\n",
        "      elif epoch == 20:\n",
        "        #SEG_LOSS_WEIGHT = 100\n",
        "        SEG_LOSS_WEIGHT = 10\n",
        "      elif epoch == 40: # after 40 epoch segmentation of CTR to BG is pretty well done => balance\n",
        "        #SEG_LOSS_WEIGHT = 10\n",
        "        SEG_LOSS_WEIGHT = min \n",
        "      elif epoch == 60: # weight to localization\n",
        "        #SEG_LOSS_WEIGHT = 10\n",
        "        pass\n",
        "\n",
        "\n",
        "  # linear function: y = 1/(epoch+1)^2 * x + min\n",
        "  elif format == 'linear':\n",
        "      print(1+abs(epoch))\n",
        "      a = 1/((1+abs(epoch))*(1+abs(epoch)))\n",
        "      SEG_LOSS_WEIGHT = a* seg + min\n",
        "\n",
        "  # exponential function\n",
        "  elif format == 'exp':\n",
        "      w = (1-abs(epoch))\n",
        "      SEG_LOSS_WEIGHT = math.exp(w*seg + min)\n",
        "\n",
        "  # fail case\n",
        "  else:\n",
        "      warnings.warn(f'NO SEG LOSS FACTOR FUNCTION DETECTED')\n",
        "      "
      ],
      "metadata": {
        "id": "WtVH7K4ZbD1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save loss from experiment run - MAY BE USED COMPARING LOSS PROPAGATION BETWEEN Networks"
      ],
      "metadata": {
        "id": "TG4s1-KscOqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_loss_propagation(loss_arr, title='Test Loss', epochs=100,save_folder=SAVE_FOLDER):\n",
        "  # make loss array as numpy\n",
        "  loss = np.asarray(loss_arr)\n",
        "  # y is the number of epochs\n",
        "  y = np.arange(epochs)\n",
        "\n",
        "  # plot settings\n",
        "  plt.title(f\"{title}\") \n",
        "  plt.xlabel(\"loss\") \n",
        "  plt.ylabel(\"Epoch\") \n",
        "  plt.plot(loss,y) \n",
        "\n",
        "  # show plot of loss\n",
        "  plt.show()\n",
        "\n",
        "  # save plot of loss to folder\n",
        "  plt.savefig(f'{SAVE_FOLDER}/loss_propagation/{title}.png')"
      ],
      "metadata": {
        "id": "DS56SR96cw-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation Algorithm and Multi-learning Algorithm"
      ],
      "metadata": {
        "id": "-T7cW6r1gD9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if EXPERIMENTAL_MODEL == 'MultiSFPNet': # Multi-learning Network with Segmentation Feedback propagation\n",
        "  class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "  # Double headed UNET\n",
        "  class UNET(nn.Module):\n",
        "    # output 3 channels (3 tubes)\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, out_channels_local=2, features=[64, 128, 256, 512], features_local=[64, 128, 256]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.downs_local = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.features = features\n",
        "        self.features_local = features_local\n",
        "        self.conv1x1 = nn.ModuleList()\n",
        "\n",
        "        self.tb = nn.ModuleList() # transfer block => fusing module for segmentation and localization information\n",
        "        self.deco_local = nn.ModuleList()\n",
        "\n",
        "        # Down part of UNET (downsampling)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        in_channels = 3\n",
        "\n",
        "        for feature in features_local:\n",
        "            self.downs_local.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        for feature in features_local: # avoid last two features\n",
        "            # avoid last feature\n",
        "            if (feature == features_local[-1]):\n",
        "              break\n",
        "            self.conv1x1.append(nn.Conv2d(feature, features_local[-1], 1))\n",
        "\n",
        "        # Up part of UNET (upsampling)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "\n",
        "        # transfer block\n",
        "        for feature in reversed(features_local):\n",
        "          if feature == 128: # only for 256 and 64 channels\n",
        "            continue\n",
        "          self.tb.append(\n",
        "              nn.ConvTranspose2d(\n",
        "                    feature, feature, kernel_size=2, stride=2\n",
        "                )\n",
        "          )\n",
        "          self.tb.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # decoding to 64 channels from 256 channels\n",
        "        self.deco_local.append(\n",
        "            nn.Conv2d(features_local[-1], features_local[0], 3, 1, 1, bias=False))\n",
        "        self.deco_local.append(\n",
        "            nn.BatchNorm2d(features_local[0]))\n",
        "        self.deco_local.append(\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "        # final 2d layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "        # double convolutions\n",
        "        self.seg_to_concat = DoubleConv(self.features[0], self.features_local[-1])\n",
        "        self.final_conv_local = DoubleConv(self.features_local[-1]*2, self.features_local[0])\n",
        "\n",
        "        # final 2d layer for localization (fully connected layer)\n",
        "        if LARGE_IMAGESIZE == True:\n",
        "          self.fc1 = nn.Linear(64*60*80, 32)\n",
        "        else:\n",
        "          self.fc1 = nn.Linear(64*15*20, 32)\n",
        "        self.fc2 = nn.Linear(32, out_channels_local)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        maps = [] # for fusing localization\n",
        "\n",
        "        #x_local = torch.clone(x.detach()) # localization\n",
        "        x_local = torch.clone(x) # localization\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        \n",
        "        # second encoder for localization\n",
        "        for down in self.downs_local:\n",
        "            x_local = down(x_local)\n",
        "            x_local = self.pool(x_local)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1] # reverse list\n",
        "\n",
        "        # decoding part\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2] # integer division\n",
        "\n",
        "            # check if their shapes does not match => makes it generalize\n",
        "            if x.shape != skip_connection.shape:\n",
        "                # do resizing\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "            #maps.append(x.detach()) # append map for usage feature channels [512, 256, 128, 64]\n",
        "            maps.append(x) # append map for usage feature channels [512, 256, 128, 64]\n",
        "\n",
        "        \n",
        "\n",
        "        # take maps and use transfer module to fuse with localization features\n",
        "        # afterwards do addition fusing of transfer module output with input from localization\n",
        "        for idx in range(0, len(self.tb), 2):\n",
        "            # clone input from localization to be used for fusing\n",
        "            local_input = torch.clone(x_local)\n",
        "\n",
        "            # apply trans conv on localization part\n",
        "            x_local = self.tb[idx](x_local)\n",
        "\n",
        "            # concatanate with seg input\n",
        "            map = maps[idx+1] # integer division # avoid working with 512, 128 channel features\n",
        "\n",
        "            # check if their shapes does not match => makes it generalize\n",
        "            if x_local.shape != map.shape:\n",
        "                # do resizing\n",
        "                x_local = TF.resize(x_local, size=map.shape[2:])\n",
        "\n",
        "            concat_map = torch.cat((map, x_local), dim=1)\n",
        "\n",
        "            # double conv\n",
        "            x_local = self.tb[idx+1](concat_map)\n",
        "\n",
        "            # resize for addition module\n",
        "            if x_local.shape != local_input.shape:\n",
        "                # do resizing\n",
        "                x_local = TF.resize(x_local, size=local_input.shape[2:])\n",
        "\n",
        "            # fusing by addition of input from localization with output of tb\n",
        "            x_local = torch.add(x_local, local_input)\n",
        "\n",
        "            # only for 256 channel output of calculation => upsampled to 64\n",
        "            if idx == 0:\n",
        "              for oper_idx in range(len(self.deco_local)):\n",
        "                x_local = self.deco_local[oper_idx](x_local)\n",
        "\n",
        "        #print(x_local.shape)\n",
        "        if LARGE_IMAGESIZE == True:\n",
        "          x_local = x_local.view(-1, 64*60*80)\n",
        "        else:\n",
        "          x_local = x_local.view(-1, 64*15*20)\n",
        "        \n",
        "\n",
        "        return [self.final_conv(x), self.fc2(self.fc1(x_local))]\n",
        "\n",
        "elif EXPERIMENTAL_MODEL == 'MultiNCONet': # Multi-Learning Network Non-Connected Output\n",
        "  \n",
        "  class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "  # Double headed UNET\n",
        "  class UNET(nn.Module):\n",
        "    # output 3 channels (3 tubes)\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, out_channels_local=2, features=[64, 128, 256, 512], features_local=[64, 128, 256]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.downs_local = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.features = features\n",
        "        self.features_local = features_local\n",
        "        self.conv1x1 = nn.ModuleList()\n",
        "\n",
        "        # Down part of UNET (downsampling)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        in_channels = 3\n",
        "\n",
        "        for feature in features_local:\n",
        "            self.downs_local.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        for feature in features_local: # avoid last two features\n",
        "            # avoid last feature\n",
        "            if (feature == features_local[-1]):\n",
        "              break\n",
        "            self.conv1x1.append(nn.Conv2d(feature, features_local[-1], 1))\n",
        "\n",
        "        # Up part of UNET (upsampling)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "        # final 2d layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "        # double convolutions\n",
        "        self.seg_to_concat = DoubleConv(self.features[0], self.features_local[-1])\n",
        "        self.final_conv_local = DoubleConv(self.features_local[-1]*2, self.features_local[0])\n",
        "\n",
        "        # final 2d layer for localization (fully connected layer)\n",
        "        self.fc1 = nn.Linear(64*60*80, 32)\n",
        "        self.fc2 = nn.Linear(32, out_channels_local)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        maps = [] # for fusing localization\n",
        "\n",
        "        #x_local = torch.clone(x) # localization\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1] # reverse list\n",
        "\n",
        "        # decoding part\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2] # integer division\n",
        "\n",
        "            # check if their shapes does not match => makes it generalize\n",
        "            if x.shape != skip_connection.shape:\n",
        "                # do resizing\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        #x_local = torch.clone(x).view(-1, 64*480*640)\n",
        "\n",
        "\n",
        "        return [self.final_conv(x), self.fc2(self.fc1(TF.resize(x, size=(60, 80)).view(-1, 64*60*80)))]\n",
        "\n",
        "elif EXPERIMENTAL_MODEL == 'DH-Unet': # Double Headed UNet\n",
        "  \n",
        "    class DoubleConv(nn.Module):\n",
        "      def __init__(self, in_channels, out_channels):\n",
        "          super(DoubleConv, self).__init__()\n",
        "          self.conv = nn.Sequential(\n",
        "              nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True),\n",
        "              nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          return self.conv(x)\n",
        "\n",
        "    # Double headed UNET\n",
        "    class UNET(nn.Module):\n",
        "      # output 3 channels (3 tubes)\n",
        "      def __init__(\n",
        "              self, in_channels=3, out_channels=1, out_channels_local=2, features=[64, 128, 256, 512], features_local=[64, 128, 256]):\n",
        "          super(UNET, self).__init__()\n",
        "          self.ups = nn.ModuleList()\n",
        "          self.downs = nn.ModuleList()\n",
        "          self.downs_local = nn.ModuleList()\n",
        "          self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "          self.features = features\n",
        "          self.features_local = features_local\n",
        "          self.conv1x1 = nn.ModuleList()\n",
        "\n",
        "          # Down part of UNET (downsampling)\n",
        "          for feature in features:\n",
        "              self.downs.append(DoubleConv(in_channels, feature))\n",
        "              in_channels = feature\n",
        "\n",
        "          in_channels = 3\n",
        "\n",
        "          for feature in features_local:\n",
        "              self.downs_local.append(DoubleConv(in_channels, feature))\n",
        "              in_channels = feature\n",
        "\n",
        "          for feature in features_local: # avoid last two features\n",
        "              # avoid last feature\n",
        "              if (feature == features_local[-1]):\n",
        "                break\n",
        "              self.conv1x1.append(nn.Conv2d(feature, features_local[-1], 1))\n",
        "\n",
        "          # Up part of UNET (upsampling)\n",
        "          for feature in reversed(features):\n",
        "              self.ups.append(\n",
        "                  nn.ConvTranspose2d(\n",
        "                      feature*2, feature, kernel_size=2, stride=2\n",
        "                  )\n",
        "              )\n",
        "              self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "          self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "          # final 2d layer\n",
        "          self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "          # double convolutions\n",
        "          self.seg_to_concat = DoubleConv(self.features[0], self.features_local[-1])\n",
        "          self.final_conv_local = DoubleConv(self.features_local[-1]*2, self.features_local[0])\n",
        "\n",
        "          # final 2d layer for localization (fully connected layer)\n",
        "          self.fc1 = nn.Linear(64*60*80, 32)\n",
        "          self.fc2 = nn.Linear(32, out_channels_local)\n",
        "\n",
        "      def forward(self, x):\n",
        "          skip_connections = []\n",
        "          maps = [] # for fusing localization\n",
        "\n",
        "          x_local = torch.clone(x) # localization\n",
        "\n",
        "          for down in self.downs:\n",
        "              x = down(x)\n",
        "              skip_connections.append(x)\n",
        "              x = self.pool(x)\n",
        "          \n",
        "          # second encoder for localization\n",
        "          for down in self.downs_local:\n",
        "              x_local = down(x_local)\n",
        "              maps.append(x_local)\n",
        "              x_local = self.pool(x_local)\n",
        "\n",
        "          x = self.bottleneck(x)\n",
        "          skip_connections = skip_connections[::-1] # reverse list\n",
        "\n",
        "          # decoding part\n",
        "          for idx in range(0, len(self.ups), 2):\n",
        "              x = self.ups[idx](x)\n",
        "              skip_connection = skip_connections[idx//2] # integer division\n",
        "\n",
        "              # check if their shapes does not match => makes it generalize\n",
        "              if x.shape != skip_connection.shape:\n",
        "                  # do resizing\n",
        "                  x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "              concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "              x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "          #x_seg = torch.clone(x)\n",
        "\n",
        "          # Second head for localization\n",
        "          # add encoder output to fused map\n",
        "          fused_map = torch.clone(x_local) # clone x_local input\n",
        "          output_seg = torch.clone(x) # clone x input\n",
        "\n",
        "          for idx in range(len(maps)-1):\n",
        "              #print(idx)\n",
        "              map = maps[idx] # get one map\n",
        "\n",
        "              map = self.pool(map)\n",
        "              map = self.conv1x1[idx](map)\n",
        "\n",
        "              # change shape to x size\n",
        "              if x_local.shape != map.shape:\n",
        "                  #print(map.shape)\n",
        "                  # do resizing on mapping to match x\n",
        "                  map = TF.resize(map, size=x_local.shape[2:])\n",
        "\n",
        "              # element wise addition to fused map\n",
        "              #print(fused_map.shape)\n",
        "              #print(map.shape)\n",
        "              fused_map = torch.add(fused_map, map)\n",
        "\n",
        "          # reset x to fused map\n",
        "          x_local = torch.clone(fused_map)\n",
        "\n",
        "          ## concatination of fused map with pre prediction output of segmentation\n",
        "\n",
        "          # downsample output of x (64 feature channels) in 256 feature channels (fused module channel size)\n",
        "          seg_pred_map = self.seg_to_concat(output_seg)\n",
        "\n",
        "          #print('seg_pred_map')\n",
        "          \n",
        "          # do concatination of fused map with pre prediction output of segmentation\n",
        "          # check if their shapes does not match => makes it generalize\n",
        "          if x_local.shape != seg_pred_map.shape:\n",
        "              # do resizing\n",
        "              #x_local = TF.resize(x_local, size=seg_pred_map.shape[2:]) #resize on output of localization\n",
        "              seg_pred_map = TF.resize(seg_pred_map, size=x_local.shape[2:]) #resize on output of segmentation\n",
        "\n",
        "          #print(seg_pred_map.shape)\n",
        "          #print(x_local.shape)\n",
        "\n",
        "          concat_local = torch.cat((seg_pred_map, x_local), dim=1)\n",
        "          #print(concat_local.shape)\n",
        "          # set output to concatination and do double conv to 64 channels\n",
        "          x_local = self.final_conv_local(concat_local)\n",
        "\n",
        "          #print(x_local.shape)\n",
        "          x_local = x_local.view(-1, 64*60*80)\n",
        "\n",
        "          return [self.final_conv(x), self.fc2(self.fc1(x_local))]\n",
        "\n",
        "else:\n",
        "  warnings.warn(f'NO MODEL WAS DETECTED WITH THE NAME: {EXPERIMENTAL_MODEL}')\n",
        "  warnings.warn(f'The available models are: MultiSFPNet  DH-Unet  MultiNCONet') \n"
      ],
      "metadata": {
        "id": "3tgCHeXIgOl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "KQxAVKgiAFn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CTRDatasetNpy(Dataset):\n",
        "    def __init__(self, images_file, masks_file, locals_file, transform=None):\n",
        "        self.images_file = images_file\n",
        "        self.masks_file = masks_file\n",
        "        self.keypoints_file = locals_file\n",
        "        self.transform = transform\n",
        "        self.images = np.load(images_file) # load numpy data of all images\n",
        "        self.masks = np.load(masks_file) # load numpy data of all masks\n",
        "        self.keypoints = np.load(locals_file) # load npy file with keypoints\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        mask = self.masks[index] # mask data is already normalized\n",
        "        keypoints = self.keypoints[index]\n",
        "\n",
        "        # The annotation is sometimes reaching edge on y coordonate because the CTR base is located there in some samples\n",
        "        for i in range(4):\n",
        "          # manage y-coordonates (idx = 1)\n",
        "          if keypoints[i][1] >= 480: \n",
        "            keypoints[i][1] = keypoints[i][1] - 1\n",
        "          # manage x-coordonates (idx = 1)\n",
        "          if keypoints[i][0] >= 640: \n",
        "            keypoints[i][0] = keypoints[i][0] - 1\n",
        "\n",
        "        if self.transform is not None: # do data augmentation\n",
        "            augmentations = self.transform(image=image, mask=mask, keypoints=keypoints)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "            keypoints = augmentations[\"keypoints\"] # outputs list object\n",
        "\n",
        "        # transform list into tensor\n",
        "        keypoints = torch.FloatTensor(keypoints)\n",
        "\n",
        "        # round transformation to closest integer\n",
        "        keypoints = torch.round(keypoints)\n",
        "        #print(keypoints)\n",
        "\n",
        "        \n",
        "        # 4 points as array of 8\n",
        "        keypoints = torch.reshape(keypoints, (-1,))\n",
        "\n",
        "        return image, mask, keypoints\n"
      ],
      "metadata": {
        "id": "xl4wtS7UAHES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "LR--ddbXANUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYPOINT_COLOR = (0, 255, 0) # Green\n",
        "\n",
        "def vis_keypoints(image, keypoints, color=KEYPOINT_COLOR, diameter=5): #(BATCH, C)\n",
        "    image = image.copy()\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # remove normalization from image\n",
        "    image = image*255\n",
        "\n",
        "    '''\n",
        "    for (x, y) in keypoints:\n",
        "        cv2.circle(image, (int(x), int(y)), diameter, (0, 255, 0), -1)\n",
        "    '''\n",
        "\n",
        "    for i in range(4):\n",
        "      cv2.circle(image, (int(keypoints[i*2]), int(keypoints[i*2+1])), diameter, KEYPOINT_COLOR, -1)\n",
        "      \n",
        "    return image\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    train_keypointdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    val_keypointdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = CTRDatasetNpy(\n",
        "        train_dir,\n",
        "        train_maskdir,\n",
        "        train_keypointdir,\n",
        "        train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = CTRDatasetNpy(\n",
        "        val_dir,\n",
        "        val_maskdir,\n",
        "        val_keypointdir,\n",
        "        val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def save_predictions_as_imgs(\n",
        "    loader, model, saved_image_index, folder=f'{SAVE_FOLDER}/output_masks/', device=\"cuda\"):\n",
        "    model.eval()\n",
        "\n",
        "    idx = -1\n",
        "    for x, y, z in loader: # image, mask, keypoints\n",
        "        idx += 1\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "\n",
        "            [output_mask, output_keypoints] = model(x)\n",
        "\n",
        "            #print(\"Pre prediction accuracy\")\n",
        "            if KEYPOINT_NORM == True:\n",
        "              output_keypoints = 0.5*torch.tanh(output_keypoints) # map between -1 1\n",
        "              \n",
        "\n",
        "\n",
        "            ### keypoints processing and saving\n",
        "            images = x.detach().cpu().numpy()\n",
        "            outputs_keypoints = output_keypoints.detach().cpu()\n",
        "            gt_keypoints = z.detach().cpu()\n",
        "\n",
        "            if KEYPOINT_NORM == True:\n",
        "              pass\n",
        "            else:\n",
        "              outputs_keypoints = outputs_keypoints.numpy()\n",
        "              gt_keypoints = gt_keypoints.numpy()\n",
        "\n",
        "\n",
        "            # save keypoint output\n",
        "            for pred_idx in range(len(outputs_keypoints)):\n",
        "                image = np.moveaxis(images[pred_idx], 0, -1)\n",
        "\n",
        "                pred_image = vis_keypoints(image, outputs_keypoints[pred_idx])\n",
        "\n",
        "                perm1 = pred_image\n",
        "\n",
        "                im_pred = Image.fromarray((perm1).astype(np.uint8))\n",
        "\n",
        "                im_pred.save(f\"{folder}/kpt_pred_epoch{saved_image_index}_batch{idx}_img{pred_idx}.png\")      \n",
        "              \n",
        "\n",
        "            # save few samples of predictions every time you save images\n",
        "            if idx == 0: # for first batch in validation\n",
        "              '''\n",
        "              for pred_idx in range(len(outputs_keypoints)):\n",
        "                image = np.moveaxis(images[pred_idx], 0, -1)\n",
        "\n",
        "                pred_image = vis_keypoints(image, outputs_keypoints[pred_idx])\n",
        "\n",
        "                perm1 = pred_image\n",
        "\n",
        "                im_pred = Image.fromarray((perm1).astype(np.uint8))\n",
        "\n",
        "                im_pred.save(f\"{folder}/pred_{saved_image_index}_{pred_idx}.png\")\n",
        "              '''\n",
        "\n",
        "              # save initially the ground truths (just the first time)\n",
        "              if saved_image_index == 0:\n",
        "                for pred_idx in range(len(gt_keypoints)):\n",
        "                  image = np.moveaxis(images[pred_idx], 0, -1)\n",
        "\n",
        "                  gt_image = vis_keypoints(image, gt_keypoints[pred_idx])\n",
        "\n",
        "                  perm2 = gt_image\n",
        "\n",
        "                  im_gt = Image.fromarray((perm2).astype(np.uint8))\n",
        "\n",
        "                  im_gt.save(f\"{folder}/groundtruth_{saved_image_index}_{pred_idx}.png\")\n",
        "\n",
        "                  torchvision.utils.save_image(y.float().unsqueeze(1), f\"{folder}/mask_{saved_image_index}.png\", normalize=True) # save mask \n",
        "\n",
        "\n",
        "\n",
        "            print(f\"The model predicted: {outputs_keypoints}; and the actual keypoints are: {gt_keypoints}\")\n",
        "            print(\"-------------------------------------------------------------------\")\n",
        "            print(\"-------------------------------------------------------------------\")\n",
        "            print(\"-------------------------------------------------------------------\")\n",
        "            print(f\"The model predicted: {outputs_keypoints[-1]}; and the actual keypoints are: {gt_keypoints[-1]}\")\n",
        "\n",
        "\n",
        "            ### mask processing and saving\n",
        "            #torchvision.utils.save_image(output_mask, f\"{folder}/mask_pred_{saved_image_index}.png\", normalize=True) # save mask\n",
        "            \n",
        "            preds1 = output_mask.detach().cpu().numpy()\n",
        "\n",
        "            #print(preds1.ndim)\n",
        "\n",
        "            for out_mask_idx in range(len(preds1)): # save mask\n",
        "                preds2 = preds1[out_mask_idx] # get last prediction from batch\n",
        "\n",
        "                #print(preds2.shape)\n",
        "\n",
        "                preds3 = preds2\n",
        "\n",
        "                seg = np.moveaxis(preds3, 0, -1)\n",
        "\n",
        "                #print(seg.shape)\n",
        "                \n",
        "                print(f\"Unique values of seg: {np.unique(seg)}\")\n",
        "\n",
        "                ## coloring\n",
        "                my_seg = seg2img(seg)\n",
        "\n",
        "                print(f\"Unique values of seg: {np.unique(my_seg)}\")\n",
        "\n",
        "                im = Image.fromarray(my_seg)\n",
        "                im.save(f\"{folder}/mask_pred_epoch{saved_image_index}_batch{idx}_img{out_mask_idx}.png\")\n",
        "            \n",
        "            print(f\"Saved prediction: {saved_image_index}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def seg2img(seg: np.array) -> np.array:\n",
        "    colours = np.array(  # Colour triplets in cv2 convention (BGR instead of RGB)\n",
        "        [[0, 0, 0],      # Black\n",
        "         [0, 0, 255],    # Red\n",
        "         [0, 255, 0],    # Green\n",
        "         [255, 0, 0]],   # Blue\n",
        "        dtype='uint8'\n",
        "    )\n",
        "\n",
        "    '''\n",
        "    print(\"The segmentation array\")\n",
        "    print(seg.ndim)\n",
        "    '''\n",
        "    \n",
        "    if seg.ndim == 2:  # assuming [H, W] containing class indices\n",
        "        if np.min(seg) < 0 or np.max(seg) > 3:\n",
        "            raise ValueError(\"Incorrect number of classes in seg array\")\n",
        "        return colours[seg]\n",
        "\n",
        "    elif seg.ndim == 3:  # assuming [H, W, C] with C containing class probabilities (logits)\n",
        "        img = seg2img(np.argmax(seg, axis=2))\n",
        "\n",
        "        # Convert image to HSV colour space to get direct access to the saturation\n",
        "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        logits = np.copy(seg)  # Copy to avoid accidentally changing a mutable array outside the function\n",
        "\n",
        "        max_vals = np.max(logits, axis=-1)  # Probability of the dominant class at each location\n",
        "\n",
        "        ind = tuple(np.indices(logits.shape[:-1])) + (logits.argmax(axis=-1),)\n",
        "\n",
        "        logits[ind] = 0  # Set all probabilities of dominant classes to 0\n",
        "\n",
        "        sec_vals = np.max(logits, axis=-1)\n",
        "\n",
        "        # Maximum probability is now that of the second-most-dominant class at each location\n",
        "        sat_vals = (max_vals - sec_vals) / max_vals\n",
        "\n",
        "        # Saturation is the lower, the closer dominant prob is to second-most-dominant prob\n",
        "        hsv_img[..., 1] = np.uint8(sat_vals * 255)  # Update image with the saturation values\n",
        "        hsv_img[..., 2] = 255\n",
        "        img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        # Now fix the black values - make gray value equal to 255-sat_val\n",
        "        indices = np.argmax(seg, axis=2) == 0\n",
        "        img[indices] = 255 * (1 - sat_vals[indices])[..., np.newaxis]\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "mAdf4dQpAO0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Utils"
      ],
      "metadata": {
        "id": "kiK7Qhh46_U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# segmentation accuracy (dice + good pixels)\n",
        "def accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 1\n",
        "    num_pixels = 1\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    mean_dist = np.zeros([4])\n",
        "\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    g_dist_err = 0\n",
        "    g_std_err = 0\n",
        "\n",
        "    validation_size = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        absolute_error_epoch = 0 # absolute error recorded for all epoch\n",
        "        for x, y, z in loader: # image, mask, keypoints\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            z = z.detach().to(device)\n",
        "\n",
        "            #print(\"Pre prediction accuracy\")\n",
        "            [output_mask, keypoints_preds] = model(x)\n",
        "\n",
        "            # apply softmax\n",
        "            output_mask = softmax(output_mask)\n",
        "\n",
        "            images = x.detach().cpu()\n",
        "            keypoints_preds = keypoints_preds.detach().cpu()\n",
        "            gt_keypoints = z.detach().cpu()\n",
        "            mask = y.detach().cpu()\n",
        "\n",
        "            validation_size += len(gt_keypoints) # add batch size\n",
        "\n",
        "            \n",
        "            #print(output_mask.shape)\n",
        "\n",
        "            #print(output_mask.shape)\n",
        "\n",
        "            output_mask = torch.transpose(output_mask, 1, 2)\n",
        "            output_mask = torch.transpose(output_mask, 2, 3)\n",
        "\n",
        "            \n",
        "            dice_score += dice_coef2(mask.numpy(), output_mask.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "            # keypoint accuracy\n",
        "            med = mean_endpoint_dist(gt_keypoints, keypoints_preds)\n",
        "\n",
        "            # overall accuracy\n",
        "            [mean_err, std_err] = med_std_keypoint_err(gt_keypoints, keypoints_preds)\n",
        "\n",
        "            # add global errors\n",
        "            if validation_size > 15:\n",
        "              g_dist_err += mean_err\n",
        "              g_std_err += std_err\n",
        "            else:\n",
        "              g_dist_err += mean_err\n",
        "              g_std_err += std_err\n",
        "\n",
        "            mean_dist += med\n",
        "\n",
        "            '''\n",
        "            for idx in range(len(med)):\n",
        "              print(f\"Keypoint (Mean Endpoint Distance) {idx}: {med[idx]} pixels (tip, tube 2, tube 1, base)\")\n",
        "              print(f\"Keypoint (Standard Deviation Endpoint) {idx}: {std[idx]} pixels (tip, tube 2, tube 1, base)\")\n",
        "            '''\n",
        "\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
        "    )\n",
        "    print(f\"Dice score: {dice_score*100/len(loader):.2f} %\")\n",
        "\n",
        "    #print(f\"mean_dist score: {mean_dist/len(loader):.2f} %\")\n",
        "\n",
        "    for idx in range(len(med)):\n",
        "      print(f\"Keypoint (Mean Endpoint Distance) {idx}: {mean_dist[idx]/validation_size:.2f} pixels (tip, tube 2, tube 1, base)\")\n",
        "\n",
        "    #print(len(loader))\n",
        "    \n",
        "    print(f\"Global Mean Err score: {g_dist_err/len(loader):.2f} pixels\")\n",
        "    print(f\"Global Std Err score: {g_std_err/len(loader):.2f} pixels\")\n",
        "    \n",
        "   \n",
        "    model.train()\n",
        "\n",
        "\n",
        "\n",
        "def dice_coef2(y_true, y_pred, no_classes=4):\n",
        "    dice=0\n",
        "\n",
        "    #print(np.unique(y_true))\n",
        "    #print(np.unique(y_pred))\n",
        "\n",
        "    # arg max from output => returns max indecise from class comp\n",
        "    max_indecies = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # comparing for each class\n",
        "    for class_index in range(no_classes):\n",
        "      # copy arrays for accidental mutations\n",
        "      logits = np.copy(max_indecies)\n",
        "      gt = np.copy(y_true)\n",
        "\n",
        "      # logical operation to filter for given class\n",
        "      pred_arr = (logits == class_index)\n",
        "      # set all values from all other classes to 0\n",
        "      gt_arr = (gt == class_index)\n",
        "\n",
        "      # two binary arrays for a specific class filtered\n",
        "      dice += dice_coef(pred_arr, gt_arr)\n",
        "\n",
        "      #print(dice)\n",
        "\n",
        "    # average dice score\n",
        "    dice = dice/no_classes\n",
        "\n",
        "    return dice\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, index=0):\n",
        "    # make 0 - 1 probabilities for specific index in output\n",
        "\n",
        "    # flatten arrays\n",
        "    y_true_f = y_true\n",
        "    y_pred_f = y_pred\n",
        "\n",
        "\n",
        "    # calculate score\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    smooth = 0.0001\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "\n",
        "# mean endpoint distance for each keypoints (for each tip, tube 2, tube 1, base)\n",
        "def mean_endpoint_dist(keypoints_gt, keypoint_preds): # for batch\n",
        "      meds = np.zeros([4])\n",
        "      keypoints_gt = keypoints_gt.float().view(len(keypoints_gt), 8) # gt is 2, 4; preds are [,8]\n",
        "      keypoint_preds = keypoint_preds.float()\n",
        "\n",
        "      # calculate endpoint distance\n",
        "      endpoint_dist = abs(keypoint_preds-keypoints_gt)\n",
        "\n",
        "      # mean endpoint distance in each batch sample\n",
        "      for batch_idx in range(len(keypoint_preds)):\n",
        "        mean_batch = []\n",
        "        for idx in range(0, 8, 2): #calculate mean for each point\n",
        "          math.pow(endpoint_dist[batch_idx][idx].float(), 2)    \n",
        "          local_mean = math.sqrt((math.pow(endpoint_dist[batch_idx][idx], 2) + math.pow(endpoint_dist[batch_idx][idx+1], 2)))\n",
        "          \n",
        "          mean_batch.append(local_mean/2)\n",
        "          print(f'Mean for sample {batch_idx}: {stat.mean(mean_batch)}')\n",
        "\n",
        "        meds += mean_batch\n",
        "\n",
        "      '''\n",
        "      print(mean_batch)\n",
        "      print(endpoint_dist)\n",
        "      print(meds)\n",
        "      '''\n",
        "\n",
        "      # return mean endpoint dist list\n",
        "      return meds\n",
        "\n",
        "\n",
        "def med_std_keypoint_err(keypoints_gt, keypoint_preds):\n",
        "  \n",
        "  # arrange tensors\n",
        "  keypoints_gt = keypoints_gt.float().view(len(keypoints_gt), 8) # gt is 2, 4; preds are [,8]\n",
        "  keypoint_preds = keypoint_preds.float()\n",
        "\n",
        "  # calculate endpoint distance from gt\n",
        "  endpoint_dist = abs(keypoint_preds-keypoints_gt)\n",
        "\n",
        "  # mean distance error\n",
        "  mean_dist_err = torch.mean(endpoint_dist)\n",
        "\n",
        "  # average standard dev error over all pixels\n",
        "  std_err = torch.std(endpoint_dist)\n",
        "\n",
        "  return [mean_dist_err, std_err]\n"
      ],
      "metadata": {
        "id": "N0zK8zld7Bll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}