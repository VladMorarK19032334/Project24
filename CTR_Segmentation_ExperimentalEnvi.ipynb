{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTR_Segmentation_ExperimentalEnvi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPymuLqSF9Ww3IEU+TCFXu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladMorarK19032334/Project24/blob/main/CTR_Segmentation_ExperimentalEnvi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dBksUtWpNLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "LM9XKwqmdkV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_AUGMENTATION == 'complex': # due to piecewise affine => different import is required\n",
        "  !python -m pip install --upgrade opencv-contrib-python\n",
        "  !pip uninstall opencv-python\n",
        "  !pip install git+https://github.com/albumentations-team/albumentations\n",
        "  !pip install opencv-python\n",
        "else:\n",
        "  !pip install albumentations==0.4.6\n",
        "\n",
        "  import albumentations as A\n",
        "  from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "f2RF9BSSdwot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import cv2\n",
        "\n",
        "import statistics as stat\n",
        "import math\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "id": "PZau9FvLdmoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimental Parameters - to decide what experiment to run for"
      ],
      "metadata": {
        "id": "lN4zncVYcs8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3gkWNGMcAiT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# True - the loss has weights on classes, False - runs the segmentation for UNet with no weights\n",
        "# available for multi-learning as well\n",
        "WEIGHTED_UNET = False \n",
        "\n",
        "\n",
        "'''\n",
        "none\t\t\tNo augmentation\n",
        "full\t\t\tThe full augmentation used in the latest models (more complex with no piecewise)\n",
        "complex\t\tFull augmentation with Piecewise Affine transformation\n",
        "'''\n",
        "MODEL_AUGMENTATION = 'full'\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4 #\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 5\n",
        "NUM_EPOCHS = 100\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 480  # original size: 480px\n",
        "IMAGE_WIDTH = 640  # original size: 640px\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "\n",
        "\n",
        "\n",
        "# weight for class segmentation in cross entropy loss\n",
        "CLASS_WEIGHTS = torch.tensor([0.5, 2.0, 1.0, 1.0]) # [bg, tip, middle, base]\n",
        "\n",
        "\n",
        "# form keypoint normalization\n",
        "KEYPOINT_NORM = False \n",
        "\n",
        "\n",
        "# training dataset for segmentation only (faster 8times less data)\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Experimental_Env/imgs_doubleheaded_extraset_v24Mar_train.npy\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Experimental_Env/masks_doubleheaded_extraset_v24Mar_train.npy\"\n",
        "TRAIN_LOCAL_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Experimental_Env/keypoints_doubleheaded_extraset_v24Mar_train.npy\"\n",
        "\n",
        "\n",
        "VAL_IMAGE_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Multi-Learning Experiments Localization/imgs_doubleheaded_x5dataset_val.npy\"\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Multi-Learning Experiments Localization/masks_doubleheaded_x5dataset_val.npy\"\n",
        "VAL_LOCAL_DIR = \"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Multi-Learning Experiments Localization/keypoints_doubleheaded_x5dataset_val.npy\"\n",
        "\n",
        "# datasets for saving all necessary visual results\n",
        "GLOBAL_FOLDER = '/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/Experimental_Env/UNetSeg' # location of envi savings\n",
        "EXPERIMENT_NAME = 'Non-WeightedSegmentation_12Apr' # name of experiment\n",
        "SAVE_FOLDER = f'{GLOBAL_FOLDER}/{EXPERIMENT_NAME}'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ0NBU6qI7Hh",
        "outputId": "ae6cab1d-d8a4-4bf8-ecc3-94b604f803a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Runner"
      ],
      "metadata": {
        "id": "Gu5XRHBwhhsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main algorithm run function\n",
        "\n",
        "def main():\n",
        "  _saved_image_index = 0\n",
        "  # select segmentation properties\n",
        "  if MODEL_AUGMENTATION == 'full': # full augmentation\n",
        "      train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Rotate(limit=90, p=1.0),\n",
        "              A.HorizontalFlip(p=0.5),\n",
        "              A.VerticalFlip(p=0.25),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "\n",
        "      val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "  elif MODEL_AUGMENTATION == 'complex': # complex augmentation\n",
        "    train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Rotate(limit=90, p=1.0),\n",
        "              A.HorizontalFlip(p=0.5),\n",
        "              A.VerticalFlip(p=0.25),\n",
        "              A.augmentations.geometric.transforms.PiecewiseAffine(scale=(0.03, 0.05), nb_rows=8, nb_cols=8, \n",
        "                                interpolation=1, mask_interpolation=0, cval=0, \n",
        "                                cval_mask=0, mode='constant', absolute_scale=False, \n",
        "                                always_apply=False, keypoints_threshold=0.01, p=0.2), # bilinear interpolation image, nearest neighbour for mask\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "  else: # none augmentation\n",
        "    train_transform = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0,\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "          [\n",
        "              A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "              A.Normalize(\n",
        "                  mean=[0.0, 0.0, 0.0],\n",
        "                  std=[1.0, 1.0, 1.0],\n",
        "                  max_pixel_value=255.0, # divide by 255\n",
        "              ),\n",
        "              ToTensorV2(),\n",
        "          ]\n",
        "      )\n",
        "    \n",
        "\n",
        "  model = UNET(in_channels=3, out_channels=4).to(DEVICE)\n",
        "  \n",
        "  # select segmentation properties\n",
        "  if WEIGHTED_UNET: # weighted classes\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(DEVICE))\n",
        "  else:\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_MASK_DIR,\n",
        "      VAL_IMAGE_DIR,\n",
        "      VAL_MASK_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transforms,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY,\n",
        "  )\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "      load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
        "  \n",
        "  accuracy(val_loader, model, device=DEVICE)\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "      print(f'EPOCH: {epoch}')\n",
        "      torch.cuda.empty_cache() # empty cuda cache\n",
        "      train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "      # save model\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "      }\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "      # check accuracy\n",
        "      accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "      # print examples every 5 epochs\n",
        "      if _saved_image_index%10 == 0 or _saved_image_index == 0 or _saved_image_index == 99:\n",
        "        # print some examples to a folder\n",
        "        save_predictions_as_imgs(\n",
        "            val_loader, model, _saved_image_index, SAVE_FOLDER, device=DEVICE\n",
        "        )\n",
        "\n",
        "      _saved_image_index += 1\n",
        "        \n",
        "\n",
        "#torch.backends.cudnn.enabled = False\n",
        "# train model\n",
        "main()\n"
      ],
      "metadata": {
        "id": "fyI7qjiMhqNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "W9frT7zgAoX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# does 1 Epoch of training\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    loop = tqdm(loader) # progress bar\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.long().to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            #targets = targets.long().to(device=DEVICE)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "FHzhldlQApWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save loss from experiment run - MAY BE USED COMPARING LOSS PROPAGATION BETWEEN Networks"
      ],
      "metadata": {
        "id": "TG4s1-KscOqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_loss_propagation(loss_arr, title='Test Loss',save_folder=SAVE_FOLDER):\n",
        "  # make loss array as numpy\n",
        "  loss = np.asarray(loss_arr)\n",
        "  # y is the number of epochs\n",
        "  y = np.arange(100)\n",
        "\n",
        "  # plot settings\n",
        "  plt.title(f\"{title}\") \n",
        "  plt.xlabel(\"loss\") \n",
        "  plt.ylabel(\"Epoch\") \n",
        "  plt.plot(loss,y) \n",
        "\n",
        "  # show plot of loss\n",
        "  plt.show()\n",
        "\n",
        "  # save plot of loss to folder\n",
        "  plt.savefig(f'{SAVE_FOLDER}/loss_propagation/{title}.png')"
      ],
      "metadata": {
        "id": "DS56SR96cw-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation Algorithm and Multi-learning Algorithm"
      ],
      "metadata": {
        "id": "-T7cW6r1gD9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "      super(DoubleConv, self).__init__()\n",
        "      self.conv = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    # output 3 channels (3 tubes)\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=4, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET (downsampling)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET (upsampling)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        # final 2d layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1] # reverse list\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2] # integer division\n",
        "\n",
        "            # check if their shapes does not match => makes it generalize\n",
        "            if x.shape != skip_connection.shape:\n",
        "                # do resizing\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n"
      ],
      "metadata": {
        "id": "3tgCHeXIgOl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "KQxAVKgiAFn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CTRDatasetNpy(Dataset):\n",
        "  def __init__(self, images_file, masks_file, transform=None):\n",
        "      self.images_file = images_file\n",
        "      self.masks_file = masks_file\n",
        "      self.transform = transform\n",
        "      self.images = np.load(images_file) # load numpy data of all images\n",
        "      self.masks = np.load(masks_file) # load numpy data of all masks\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image = self.images[index]\n",
        "      mask = self.masks[index] # mask data is already normalized\n",
        "\n",
        "      if self.transform is not None: # do data augmentation\n",
        "          augmentations = self.transform(image=image, mask=mask)\n",
        "          image = augmentations[\"image\"]\n",
        "          mask = augmentations[\"mask\"]\n",
        "\n",
        "      return image, mask\n"
      ],
      "metadata": {
        "id": "xl4wtS7UAHES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "LR--ddbXANUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYPOINT_COLOR = (255, 0, 0) # Green\n",
        "\n",
        "def vis_keypoints(image, keypoints, color=KEYPOINT_COLOR, diameter=5): #(BATCH, C)\n",
        "    image = image.copy()\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # remove normalization from image\n",
        "    image = image*255\n",
        "\n",
        "    '''\n",
        "    for (x, y) in keypoints:\n",
        "        cv2.circle(image, (int(x), int(y)), diameter, (0, 255, 0), -1)\n",
        "    '''\n",
        "\n",
        "    for i in range(4):\n",
        "      cv2.circle(image, (int(keypoints[i*2]), int(keypoints[i*2+1])), diameter, KEYPOINT_COLOR, -1)\n",
        "      \n",
        "    return image\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = CTRDatasetNpy(\n",
        "        train_dir,\n",
        "        train_maskdir,\n",
        "        train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = CTRDatasetNpy(\n",
        "        val_dir,\n",
        "        val_maskdir,\n",
        "        val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "def save_predictions_as_imgs(\n",
        "    loader, model, saved_image_index, folder=\"/content/drive/MyDrive/Biomed 3rd Year/BEng Project/Training Datasets/CTRvBG_Multichannel_v01/UNet_Aug_OutputSizeColor_v02/\", device=\"cuda\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = model(x)\n",
        "\n",
        "            # *** NEW *** set colors based on probabilities\n",
        "\n",
        "            print(preds.shape)\n",
        "            preds1 = preds.detach().cpu().numpy()\n",
        "\n",
        "            for out_mask_idx in range(len(preds)): # save mask\n",
        "                preds2 = preds1[out_mask_idx] # get last prediction from batch\n",
        "\n",
        "                #print(preds2.shape)\n",
        "\n",
        "                preds3 = preds2\n",
        "\n",
        "                seg = np.moveaxis(preds3, 0, -1)\n",
        "\n",
        "                #print(seg.shape)\n",
        "                \n",
        "                #print(f\"Unique values of seg: {np.unique(seg)}\")\n",
        "\n",
        "                ## coloring\n",
        "                my_seg = seg2img(seg)\n",
        "\n",
        "                #print(f\"Unique values of seg: {np.unique(my_seg)}\")\n",
        "\n",
        "                im = Image.fromarray(my_seg)\n",
        "                im.save(f\"{folder}/mask_pred_epoch{saved_image_index}_batch{idx}_img{out_mask_idx}.png\")\n",
        "            \n",
        "            print(f\"Saved prediction: {saved_image_index}\")\n",
        "        # ERROR LINE\n",
        "        if saved_image_index == 0: # save ground truth for one epoch only\n",
        "          #torchvision.utils.save_image(x.unsqueeze(1), f\"{folder}/image_{saved_image_index}_{idx}.png\", normalize=True) # save image\n",
        "          torchvision.utils.save_image(y.float().unsqueeze(1), f\"{folder}/mask_{saved_image_index}_{idx}.png\", normalize=True) # save mask\n",
        "          print(f\"Saved image: {saved_image_index}\")\n",
        "\n",
        "    model.train()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def seg2img(seg: np.array) -> np.array:\n",
        "    colours = np.array(  # Colour triplets in cv2 convention (BGR instead of RGB)\n",
        "        [[0, 0, 0],      # Black\n",
        "         [0, 0, 255],    # Red\n",
        "         [0, 255, 0],    # Green\n",
        "         [255, 0, 0]],   # Blue\n",
        "        dtype='uint8'\n",
        "    )\n",
        "\n",
        "    '''\n",
        "    print(\"The segmentation array\")\n",
        "    print(seg.ndim)\n",
        "    '''\n",
        "    \n",
        "    if seg.ndim == 2:  # assuming [H, W] containing class indices\n",
        "        if np.min(seg) < 0 or np.max(seg) > 3:\n",
        "            raise ValueError(\"Incorrect number of classes in seg array\")\n",
        "        return colours[seg]\n",
        "\n",
        "    elif seg.ndim == 3:  # assuming [H, W, C] with C containing class probabilities (logits)\n",
        "        img = seg2img(np.argmax(seg, axis=2))\n",
        "\n",
        "        # Convert image to HSV colour space to get direct access to the saturation\n",
        "        hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        logits = np.copy(seg)  # Copy to avoid accidentally changing a mutable array outside the function\n",
        "\n",
        "        max_vals = np.max(logits, axis=-1)  # Probability of the dominant class at each location\n",
        "\n",
        "        ind = tuple(np.indices(logits.shape[:-1])) + (logits.argmax(axis=-1),)\n",
        "\n",
        "        logits[ind] = 0  # Set all probabilities of dominant classes to 0\n",
        "\n",
        "        sec_vals = np.max(logits, axis=-1)\n",
        "\n",
        "        # Maximum probability is now that of the second-most-dominant class at each location\n",
        "        sat_vals = (max_vals - sec_vals) / max_vals\n",
        "\n",
        "        # Saturation is the lower, the closer dominant prob is to second-most-dominant prob\n",
        "        hsv_img[..., 1] = np.uint8(sat_vals * 255)  # Update image with the saturation values\n",
        "        hsv_img[..., 2] = 255\n",
        "        img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        # Now fix the black values - make gray value equal to 255-sat_val\n",
        "        indices = np.argmax(seg, axis=2) == 0\n",
        "        img[indices] = 255 * (1 - sat_vals[indices])[..., np.newaxis]\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "mAdf4dQpAO0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Utils"
      ],
      "metadata": {
        "id": "kiK7Qhh46_U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# segmentation accuracy (dice + good pixels)\n",
        "def accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 1\n",
        "    num_pixels = 1\n",
        "    dice_score = 0\n",
        "    conf_dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        absolute_error_epoch = 0 # absolute error recorded for all epoch\n",
        "        for x, y in loader: # image, mask, keypoints\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            #print(\"Pre prediction accuracy\")\n",
        "            # apply softmax on output\n",
        "            output_mask = softmax(model(x))\n",
        "\n",
        "            images = x.detach().cpu()\n",
        "            mask = y.detach().cpu()\n",
        "            \n",
        "            #print(output_mask.shape)\n",
        "\n",
        "            output_mask = torch.transpose(output_mask, 1, 2)\n",
        "            output_mask = torch.transpose(output_mask, 2, 3)\n",
        "\n",
        "            x, y = dice_coef2(mask.numpy(), output_mask.detach().cpu().numpy())\n",
        "            dice_score += x\n",
        "            conf_dice_score += y\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
        "    )\n",
        "    print(f\"Dice score: {dice_score*100/len(loader):.2f} %\")\n",
        "    print(f\"CONFIDENCE Dice score: {conf_dice_score*100/len(loader):.2f} %\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, index=0):\n",
        "    \n",
        "    '''\n",
        "    # y_true = torch.Tensor(y_true)\n",
        "    y_pred = torch.Tensor(y_pred)\n",
        "\n",
        "\n",
        "    # make 0 - 1 probabilities for specific index in output\n",
        "    y_pred = (y_pred > 0.5).float().numpy()\n",
        "    '''\n",
        "\n",
        "    # flatten arrays\n",
        "    y_true_f = y_true\n",
        "    y_pred_f = y_pred\n",
        "\n",
        "\n",
        "    # calculate score\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    smooth = 0.0001\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_multilabel(y_true, y_pred, numLabels):\n",
        "    dice=0\n",
        "\n",
        "    print(y_true.size)\n",
        "    print(y_pred.size)\n",
        "\n",
        "    for index in range(numLabels):\n",
        "        dice += dice_coef(y_true, y_pred, index)\n",
        "    return dice/numLabels # taking average\n",
        "\n",
        "\n",
        "def dice_coef2(y_true, y_pred, no_classes=4):\n",
        "    dice=0\n",
        "    conf_dice = 0\n",
        "\n",
        "    #print(np.unique(y_true))\n",
        "    #print(np.unique(y_pred))\n",
        "\n",
        "    # arg max from output => returns max indecise from class comp\n",
        "    max_indecies = np.argmax(y_pred, axis=-1)\n",
        "    probs = np.copy(y_pred)\n",
        "    max_values = np.max(probs, axis=-1)  # Probability of the dominant class at each location\n",
        "\n",
        "    # comparing for each class\n",
        "    for class_index in range(no_classes):\n",
        "      # copy arrays for accidental mutations\n",
        "      logits = np.copy(max_indecies)\n",
        "      gt = np.copy(y_true)\n",
        "\n",
        "      # logical operation to filter for given class\n",
        "      pred_arr = (logits == class_index)\n",
        "      # set all values from all other classes to 0\n",
        "      gt_arr = (gt == class_index)\n",
        "\n",
        "      # two binary arrays for a specific class filtered\n",
        "      dice += dice_coef(gt_arr, pred_arr)\n",
        "\n",
        "      # confidence dice score\n",
        "      conf_dice += confidence_dice(gt_arr, pred_arr, max_values)\n",
        "\n",
        "      #print(dice)\n",
        "\n",
        "    # average dice score\n",
        "    dice = dice/no_classes\n",
        "    conf_dice = conf_dice/no_classes\n",
        "\n",
        "    return dice, conf_dice\n",
        "\n",
        "\n",
        "# implements the idea of how sure the model is about the class in the score\n",
        "def confidence_dice(y_true, indecies, logits):\n",
        "    # flatten arrays\n",
        "    y_true_f = y_true\n",
        "    y_pred_f = indecies\n",
        "\n",
        "    # calculate score\n",
        "    intersection = np.sum(y_true_f * y_pred_f * logits)\n",
        "    smooth = 0.0001\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n"
      ],
      "metadata": {
        "id": "N0zK8zld7Bll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}